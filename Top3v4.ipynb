{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBSk0-x9iN-m",
    "outputId": "4c7ba434-62ef-406e-fff2-27838e450251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFO8n6SnZNog",
    "outputId": "056b6fcf-720a-4686-8c2f-6d5217500925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers torchaudio torchcodec\n",
    "!apt install ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LuOu4OdBieew",
    "outputId": "86dfd0d6-0fdc-4587-c2f7-566cf34a5069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 300 .wav files. Using: /content/drive/MyDrive/adversarial-audio/Normal-Examples/long-signals/sample-070236.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "AUDIO_DIR = \"/content/drive/MyDrive/adversarial-audio/Normal-Examples/long-signals\"\n",
    "TARGET_SR = 16000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def pick_sample(audio_dir=AUDIO_DIR):\n",
    "    wavs = sorted(glob.glob(os.path.join(audio_dir, \"*.wav\")))\n",
    "    if not wavs:\n",
    "        raise FileNotFoundError(f\"No .wav files found in {audio_dir}. Update AUDIO_DIR to your folder.\")\n",
    "    print(f\"Found {len(wavs)} .wav files. Using: {wavs[0]}\")\n",
    "    return wavs[0]\n",
    "\n",
    "SAMPLE_FILE = pick_sample()\n",
    "# To target a specific file instead of the first one, uncomment and set the path below:\n",
    "# SAMPLE_FILE = os.path.join(AUDIO_DIR, \"your-file.wav\")\n",
    "\n",
    "def load_audio(path, target_sr=TARGET_SR):\n",
    "    waveform, sr = torchaudio.load(path)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # downmix to mono\n",
    "    if sr != target_sr:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, target_sr)\n",
    "    return waveform.to(device)\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHPgwXWnYD63"
   },
   "source": [
    "PGD-Based White-Box Non-Targeted Attack for ASR (Gao et al. (2024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cq__-v3bW6yp",
    "outputId": "e63d20a1-2d26-418c-ca7f-70cb2ffa5875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PGD adversarial example to /content/adv_sample_pgd.wav\n"
     ]
    }
   ],
   "source": [
    "# pgd_attack.py\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torchaudio\n",
    "\n",
    "class PGDAudioAttack:\n",
    "    def __init__(self, model, processor, epsilon=0.002, alpha=0.0004, steps=10, sample_rate=16000, device=None):\n",
    "        self.model = model.eval()\n",
    "        self.processor = processor  # kept for consistency/decoding if needed\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.steps = steps\n",
    "        self.sample_rate = sample_rate\n",
    "        self.device = device or next(model.parameters()).device\n",
    "\n",
    "    def _prepare_audio(self, audio_tensor):\n",
    "        # Flatten to mono 1D and add batch dim -> shape [1, time]\n",
    "        if audio_tensor.dim() == 0:\n",
    "            raise ValueError(\"Audio tensor must have at least 1 dimension\")\n",
    "        audio = audio_tensor\n",
    "        if audio.dim() > 1:\n",
    "            audio = audio.mean(dim=0)\n",
    "        audio = audio.view(-1)  # ensure 1D\n",
    "        return audio.unsqueeze(0).to(self.device)\n",
    "\n",
    "    def _normalize(self, audio_batch):\n",
    "        # Simple zero-mean, peak normalization in torch to keep gradients.\n",
    "        audio = audio_batch - audio_batch.mean(dim=1, keepdim=True)\n",
    "        peak = audio.abs().max(dim=1, keepdim=True).values.clamp(min=1e-6)\n",
    "        return audio / peak\n",
    "\n",
    "    def forward(self, audio_tensor):\n",
    "        base_audio = self._prepare_audio(audio_tensor).detach()\n",
    "        perturbed = base_audio.clone().detach().requires_grad_(True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            orig_logits = self.model(self._normalize(base_audio)).logits\n",
    "            orig_pred = torch.argmax(orig_logits, dim=-1)\n",
    "\n",
    "        for _ in range(self.steps):\n",
    "            norm_audio = self._normalize(perturbed)\n",
    "            logits = self.model(norm_audio).logits\n",
    "            log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "            input_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=self.device)\n",
    "            target_lengths = torch.full((orig_pred.size(0),), orig_pred.size(1), dtype=torch.long, device=self.device)\n",
    "\n",
    "            loss = -F.ctc_loss(log_probs.transpose(0, 1), orig_pred,\n",
    "                               input_lengths=input_lengths,\n",
    "                               target_lengths=target_lengths,\n",
    "                               blank=0, reduction='mean', zero_infinity=True)\n",
    "\n",
    "            loss.backward()\n",
    "            grad_sign = perturbed.grad.sign()\n",
    "            perturbed = perturbed + self.alpha * grad_sign\n",
    "            perturbation = torch.clamp(perturbed - base_audio, min=-self.epsilon, max=self.epsilon)\n",
    "            perturbed = torch.clamp(base_audio + perturbation, min=-1.0, max=1.0).detach().requires_grad_(True)\n",
    "\n",
    "        return perturbed.detach()\n",
    "\n",
    "\n",
    "waveform = load_audio(SAMPLE_FILE)\n",
    "\n",
    "attacker = PGDAudioAttack(model, processor, sample_rate=TARGET_SR, device=device)\n",
    "adv_audio = attacker.forward(waveform)\n",
    "\n",
    "torchaudio.save(\"/content/adv_sample_pgd.wav\", adv_audio.cpu(), TARGET_SR)\n",
    "print(\"Saved PGD adversarial example to /content/adv_sample_pgd.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrnfI5vgYIZX"
   },
   "source": [
    "Imperceptible White-Box ASR Attack Using Psychoacoustic Masking (Abdullah et al. (2021))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "0HlAgwZTX-mI",
    "outputId": "848c826d-d604-476c-c0cf-9af927e44d94"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 105600]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3023131965.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mattacker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImperceptibleAttack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_SR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0madv_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattacker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/adv_imperceptible.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_audio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_SR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3023131965.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, audio_tensor)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mnorm_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturbed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_audio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1860\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Label values must be <= vocab_size: {self.config.vocab_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         outputs = self.wav2vec2(\n\u001b[0m\u001b[1;32m   1863\u001b[0m             \u001b[0minput_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mextract_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m         \u001b[0mextract_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[0;32m--> 366\u001b[0;31m         return F.conv1d(\n\u001b[0m\u001b[1;32m    367\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 105600]"
     ]
    }
   ],
   "source": [
    "# imperceptible_attack.py\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.nn import functional as F\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "class ImperceptibleAttack:\n",
    "    def __init__(self, model, processor, epsilon=0.002, alpha=0.0003, steps=15, sample_rate=16000, masking_db=20, device=None):\n",
    "        self.model = model.eval()\n",
    "        self.processor = processor\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.steps = steps\n",
    "        self.sample_rate = sample_rate\n",
    "        self.masking_db = masking_db\n",
    "        self.device = device or next(model.parameters()).device\n",
    "        self.melspec = T.MelSpectrogram(sample_rate=sample_rate, n_mels=64).to(self.device)\n",
    "\n",
    "    def _prepare_audio(self, audio_tensor):\n",
    "        # Flatten to mono 1D and add batch dim -> shape [1, time]\n",
    "        if audio_tensor.dim() == 0:\n",
    "            raise ValueError(\"Audio tensor must have at least 1 dimension\")\n",
    "        audio = audio_tensor\n",
    "        if audio.dim() > 1:\n",
    "            audio = audio.mean(dim=0)\n",
    "        audio = audio.view(-1)\n",
    "        return audio.unsqueeze(0).to(self.device)\n",
    "\n",
    "    def _normalize(self, audio_batch):\n",
    "        # Zero-mean, peak-normalize to keep values in [-1, 1].\n",
    "        audio = audio_batch - audio_batch.mean(dim=1, keepdim=True)\n",
    "        peak = audio.abs().max(dim=1, keepdim=True).values.clamp(min=1e-6)\n",
    "        return audio / peak\n",
    "\n",
    "    def masking_threshold(self, audio):\n",
    "        # Generate a crude auditory masking threshold.\n",
    "        with torch.no_grad():\n",
    "            spec = self.melspec(audio)\n",
    "            threshold = spec.mean(dim=-1, keepdim=True) - self.masking_db\n",
    "            mask = (spec >= threshold).float()\n",
    "        return mask\n",
    "\n",
    "    def apply_mask(self, grad, mask):\n",
    "        # Scale gradients based on Mel mask energy. Interpolate mask to waveform length.\n",
    "        # mask shape: [batch, n_mels, time_frames]\n",
    "        # grad shape: [batch, time]\n",
    "        mel_mask = mask.mean(dim=1)  # [batch, time_frames] - average across mel bins\n",
    "        # Interpolate to match grad time dimension\n",
    "        if mel_mask.dim() == 2:\n",
    "            mel_mask = mel_mask.unsqueeze(1)  # [batch, 1, time_frames] for interpolate\n",
    "        mel_mask = F.interpolate(mel_mask, size=grad.shape[-1], mode='linear', align_corners=False)\n",
    "        mel_mask = mel_mask.squeeze(1)  # [batch, time] to match grad\n",
    "        return grad * mel_mask\n",
    "\n",
    "    def forward(self, audio_tensor):\n",
    "        base_audio = self._prepare_audio(audio_tensor).detach()\n",
    "        perturbed = base_audio.clone().detach().requires_grad_(True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            orig_logits = self.model(self._normalize(base_audio)).logits\n",
    "            orig_pred = torch.argmax(orig_logits, dim=-1)\n",
    "\n",
    "        masking_mask = self.masking_threshold(base_audio)\n",
    "\n",
    "        for _ in range(self.steps):\n",
    "            norm_audio = self._normalize(perturbed)\n",
    "            logits = self.model(norm_audio).logits\n",
    "            log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "            input_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=self.device)\n",
    "            target_lengths = torch.full((orig_pred.size(0),), orig_pred.size(1), dtype=torch.long, device=self.device)\n",
    "\n",
    "            loss = -F.ctc_loss(log_probs.transpose(0, 1), orig_pred,\n",
    "                               input_lengths=input_lengths,\n",
    "                               target_lengths=target_lengths,\n",
    "                               blank=0, reduction='mean', zero_infinity=True)\n",
    "            loss.backward()\n",
    "\n",
    "            grad_sign = perturbed.grad.sign()\n",
    "            grad_masked = self.apply_mask(grad_sign, masking_mask)\n",
    "            perturbed = perturbed + self.alpha * grad_masked\n",
    "            perturbation = torch.clamp(perturbed - base_audio, min=-self.epsilon, max=self.epsilon)\n",
    "            perturbed = torch.clamp(base_audio + perturbation, min=-1.0, max=1.0).detach().requires_grad_(True)\n",
    "\n",
    "        return perturbed.detach()\n",
    "\n",
    "\n",
    "waveform = load_audio(SAMPLE_FILE)\n",
    "\n",
    "attacker = ImperceptibleAttack(model, processor, sample_rate=TARGET_SR, device=device)\n",
    "adv_audio = attacker.forward(waveform)\n",
    "\n",
    "torchaudio.save(\"/content/adv_imperceptible.wav\", adv_audio.cpu(), TARGET_SR)\n",
    "print(\"Saved imperceptible adversarial example to /content/adv_imperceptible.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zol6U5clYTXk"
   },
   "source": [
    "AdvReverb: Convolutional White-Box Adversarial Attack (Chen et al. (2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuEYaxxTYWPs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "class AdvReverbAttack:\n",
    "    def __init__(self, model, processor, rir_len=2048, lr=1e-3, steps=300, sample_rate=16000, device=None):\n",
    "        self.model = model.eval()\n",
    "        self.processor = processor\n",
    "        self.rir_len = rir_len\n",
    "        self.lr = lr\n",
    "        self.steps = steps\n",
    "        self.sample_rate = sample_rate\n",
    "        self.device = device or next(model.parameters()).device\n",
    "\n",
    "    def _prepare_audio(self, audio_tensor):\n",
    "        # Ensure a single-channel batch tensor shaped [1, time]\n",
    "        audio = audio_tensor\n",
    "        if audio.dim() == 3:  # [batch, channel, time]\n",
    "            audio = audio.mean(dim=1)  # [batch, time]\n",
    "        elif audio.dim() == 2:  # could be [channels, time] or [batch, time]\n",
    "            if audio.size(0) > 1:\n",
    "                audio = audio.mean(dim=0, keepdim=True)  # [1, time]\n",
    "            # If already [1, time], keep it as is\n",
    "        elif audio.dim() == 1:\n",
    "            audio = audio.unsqueeze(0)  # [1, time]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected audio tensor shape: {audio.shape}\")\n",
    "        \n",
    "        # Ensure final shape is exactly [1, time]\n",
    "        if audio.dim() != 2 or audio.size(0) != 1:\n",
    "            # Flatten to 1D and add batch dimension\n",
    "            audio = audio.view(-1).unsqueeze(0)\n",
    "        \n",
    "        return audio.to(self.device)\n",
    "    \n",
    "    def _normalize(self, audio_batch):\n",
    "        # Zero-mean, peak-normalize to keep values in [-1, 1].\n",
    "        audio = audio_batch - audio_batch.mean(dim=1, keepdim=True)\n",
    "        peak = audio.abs().max(dim=1, keepdim=True).values.clamp(min=1e-6)\n",
    "        return audio / peak\n",
    "\n",
    "    def forward(self, audio_tensor):\n",
    "        base_audio = self._prepare_audio(audio_tensor).detach()  # [1, time]\n",
    "\n",
    "        rir = torch.nn.Parameter(torch.randn(self.rir_len, device=self.device) * 1e-3)\n",
    "        optimizer = torch.optim.Adam([rir], lr=self.lr)\n",
    "\n",
    "        # Get original prediction using direct model input (not processor)\n",
    "        with torch.no_grad():\n",
    "            base_audio_norm = self._normalize(base_audio)\n",
    "            orig_logits = self.model(base_audio_norm).logits\n",
    "            orig_pred = torch.argmax(orig_logits, dim=-1)\n",
    "\n",
    "        for _ in range(self.steps):\n",
    "            optimizer.zero_grad()\n",
    "            rir_windowed = rir * torch.hann_window(self.rir_len, device=self.device)\n",
    "            rir_norm = rir_windowed / torch.norm(rir_windowed, p=2)\n",
    "\n",
    "            # Apply convolution: base_audio is [1, time], unsqueeze to [1, 1, time]\n",
    "            adv_audio = torch.nn.functional.conv1d(base_audio.unsqueeze(1), rir_norm.view(1, 1, -1), padding=self.rir_len // 2)\n",
    "            # adv_audio is [1, 1, time], squeeze to [1, time]\n",
    "            adv_audio = adv_audio.squeeze(1)\n",
    "            adv_audio_norm = self._normalize(adv_audio)\n",
    "\n",
    "            # Pass normalized audio directly to model\n",
    "            logits = self.model(adv_audio_norm).logits\n",
    "            log_probs = logits.log_softmax(dim=-1)\n",
    "\n",
    "            input_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=self.device)\n",
    "            target_lengths = torch.full((orig_pred.size(0),), orig_pred.size(1), dtype=torch.long, device=self.device)\n",
    "\n",
    "            loss = -F.ctc_loss(log_probs.transpose(0, 1), orig_pred,\n",
    "                               input_lengths=input_lengths,\n",
    "                               target_lengths=target_lengths,\n",
    "                               blank=0, reduction='mean', zero_infinity=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Final convolution with optimized RIR\n",
    "        with torch.no_grad():\n",
    "            final_rir = rir * torch.hann_window(self.rir_len, device=self.device)\n",
    "            final_rir = final_rir / torch.norm(final_rir, p=2)\n",
    "            # base_audio is [1, time], unsqueeze to [1, 1, time]\n",
    "            adv_audio = torch.nn.functional.conv1d(base_audio.unsqueeze(1), final_rir.view(1, 1, -1), padding=self.rir_len // 2)\n",
    "            # Squeeze to [1, time]\n",
    "            adv_audio = adv_audio.squeeze(1)\n",
    "\n",
    "        return adv_audio.detach(), final_rir.detach()\n",
    "\n",
    "\n",
    "waveform = load_audio(SAMPLE_FILE)\n",
    "\n",
    "attacker = AdvReverbAttack(model, processor, sample_rate=TARGET_SR, device=device)\n",
    "adv_audio, adv_rir = attacker.forward(waveform)\n",
    "\n",
    "torchaudio.save(\"/content/adv_reverb.wav\", adv_audio.cpu(), TARGET_SR)\n",
    "print(\"Saved reverb adversarial example to /content/adv_reverb.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional dependencies for metrics and ASR\n",
    "%pip install -q pesq pystoi openai-whisper librosa scipy soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics computation functions\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import tempfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, Optional\n",
    "import whisper\n",
    "from pesq import pesq as pesq_metric\n",
    "from pystoi.stoi import stoi as stoi_metric\n",
    "import librosa\n",
    "\n",
    "# Initialize Whisper model\n",
    "print(\"Loading Whisper model...\")\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "print(\"Whisper model loaded!\")\n",
    "\n",
    "def compute_snr(original: np.ndarray, degraded: np.ndarray) -> float:\n",
    "    \"\"\"Calculate Signal-to-Noise Ratio (SNR) in dB.\"\"\"\n",
    "    # Ensure same length by trimming to minimum\n",
    "    min_len = min(len(original), len(degraded))\n",
    "    original = original[:min_len]\n",
    "    degraded = degraded[:min_len]\n",
    "    \n",
    "    signal_power = np.mean(original ** 2)\n",
    "    noise_power = np.mean((degraded - original) ** 2)\n",
    "    if noise_power == 0:\n",
    "        return float('inf')\n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return float(snr)\n",
    "\n",
    "def compute_pesq(reference: np.ndarray, degraded: np.ndarray, sr: int = TARGET_SR) -> float:\n",
    "    \"\"\"Compute PESQ (ITU-T P.862). Uses wideband mode for 16 kHz.\"\"\"\n",
    "    min_len = min(len(reference), len(degraded))\n",
    "    reference = reference[:min_len]\n",
    "    degraded = degraded[:min_len]\n",
    "    try:\n",
    "        return float(pesq_metric(sr, reference, degraded, 'wb'))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def compute_stoi(reference: np.ndarray, degraded: np.ndarray, sr: int = TARGET_SR) -> float:\n",
    "    \"\"\"Compute STOI (0..1).\"\"\"\n",
    "    min_len = min(len(reference), len(degraded))\n",
    "    reference = reference[:min_len]\n",
    "    degraded = degraded[:min_len]\n",
    "    try:\n",
    "        return float(stoi_metric(reference, degraded, sr, extended=False))\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def compute_wer_cer(reference_text: str, hypothesis_text: str) -> Tuple[float, float]:\n",
    "    \"\"\"Compute Word Error Rate (WER) and Character Error Rate (CER).\"\"\"\n",
    "    ref_words = reference_text.lower().split()\n",
    "    hyp_words = hypothesis_text.lower().split()\n",
    "    \n",
    "    # WER using Levenshtein distance on words\n",
    "    if len(ref_words) == 0:\n",
    "        wer = 1.0 if len(hyp_words) > 0 else 0.0\n",
    "    else:\n",
    "        # Simple word-level edit distance\n",
    "        from difflib import SequenceMatcher\n",
    "        matcher = SequenceMatcher(None, ref_words, hyp_words)\n",
    "        wer = 1.0 - matcher.ratio()\n",
    "    \n",
    "    # CER\n",
    "    ref_chars = list(reference_text.lower().replace(\" \", \"\"))\n",
    "    hyp_chars = list(hypothesis_text.lower().replace(\" \", \"\"))\n",
    "    if len(ref_chars) == 0:\n",
    "        cer = 1.0 if len(hyp_chars) > 0 else 0.0\n",
    "    else:\n",
    "        from difflib import SequenceMatcher\n",
    "        matcher = SequenceMatcher(None, ref_chars, hyp_chars)\n",
    "        cer = 1.0 - matcher.ratio()\n",
    "    \n",
    "    return float(wer), float(cer)\n",
    "\n",
    "def transcribe_audio(audio_array: np.ndarray, sr: int = TARGET_SR) -> str:\n",
    "    \"\"\"Transcribe audio using Whisper.\"\"\"\n",
    "    # Save to temporary file for Whisper\n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:\n",
    "        tmp_path = tmp_file.name\n",
    "        import soundfile as sf\n",
    "        sf.write(tmp_path, audio_array, sr)\n",
    "    \n",
    "    try:\n",
    "        result = whisper_model.transcribe(tmp_path)\n",
    "        transcript = result[\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Whisper transcription error: {e}\")\n",
    "        transcript = \"\"\n",
    "    finally:\n",
    "        import os\n",
    "        if os.path.exists(tmp_path):\n",
    "            os.remove(tmp_path)\n",
    "    \n",
    "    return transcript\n",
    "\n",
    "def compute_all_metrics(original_audio: np.ndarray, processed_audio: np.ndarray, \n",
    "                        original_transcript: Optional[str] = None, \n",
    "                        sr: int = TARGET_SR) -> Dict[str, float]:\n",
    "    \"\"\"Compute all metrics: SNR, PESQ, STOI, WER, CER.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Audio quality metrics\n",
    "    metrics['snr'] = compute_snr(original_audio, processed_audio)\n",
    "    metrics['pesq'] = compute_pesq(original_audio, processed_audio, sr)\n",
    "    metrics['stoi'] = compute_stoi(original_audio, processed_audio, sr)\n",
    "    \n",
    "    # ASR metrics (if transcript provided)\n",
    "    if original_transcript:\n",
    "        processed_transcript = transcribe_audio(processed_audio, sr)\n",
    "        wer, cer = compute_wer_cer(original_transcript, processed_transcript)\n",
    "        metrics['wer'] = wer\n",
    "        metrics['cer'] = cer\n",
    "        metrics['transcript'] = processed_transcript\n",
    "    else:\n",
    "        # Get transcript from original audio\n",
    "        original_transcript = transcribe_audio(original_audio, sr)\n",
    "        processed_transcript = transcribe_audio(processed_audio, sr)\n",
    "        wer, cer = compute_wer_cer(original_transcript, processed_transcript)\n",
    "        metrics['wer'] = wer\n",
    "        metrics['cer'] = cer\n",
    "        metrics['transcript'] = processed_transcript\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Metrics functions ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression functions using ffmpeg\n",
    "import soundfile as sf\n",
    "\n",
    "def compress_opus(audio_array: np.ndarray, output_path: str, sr: int = TARGET_SR, bitrate: int = 64) -> np.ndarray:\n",
    "    \"\"\"Compress audio to Opus format and decode back to numpy array.\"\"\"\n",
    "    # Save input as temporary WAV\n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_in:\n",
    "        tmp_in_path = tmp_in.name\n",
    "        sf.write(tmp_in_path, audio_array, sr)\n",
    "    \n",
    "    # Compress to Opus\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',\n",
    "            '-i', tmp_in_path,\n",
    "            '-codec:a', 'libopus',\n",
    "            '-b:a', f'{bitrate}k',\n",
    "            output_path\n",
    "        ], check=True, capture_output=True)\n",
    "        \n",
    "        # Decode back to WAV\n",
    "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_out:\n",
    "            tmp_out_path = tmp_out.name\n",
    "            subprocess.run([\n",
    "                'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',\n",
    "                '-i', output_path,\n",
    "                tmp_out_path\n",
    "            ], check=True, capture_output=True)\n",
    "            \n",
    "            # Load decoded audio\n",
    "            decoded_audio, decoded_sr = librosa.load(tmp_out_path, sr=sr, mono=True)\n",
    "            \n",
    "            # Cleanup\n",
    "            import os\n",
    "            if os.path.exists(tmp_out_path):\n",
    "                os.remove(tmp_out_path)\n",
    "            if os.path.exists(tmp_in_path):\n",
    "                os.remove(tmp_in_path)\n",
    "            \n",
    "            return decoded_audio\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Opus compression error: {e}\")\n",
    "        import os\n",
    "        if os.path.exists(tmp_in_path):\n",
    "            os.remove(tmp_in_path)\n",
    "        return audio_array  # Return original on error\n",
    "\n",
    "def compress_amr(audio_array: np.ndarray, output_path: str, sr: int = TARGET_SR, bitrate: float = 12.65) -> np.ndarray:\n",
    "    \"\"\"Compress audio to AMR-WB format and decode back to numpy array.\n",
    "    Falls back to AAC if AMR-WB encoder is not available.\"\"\"\n",
    "    # Save input as temporary WAV\n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_in:\n",
    "        tmp_in_path = tmp_in.name\n",
    "        sf.write(tmp_in_path, audio_array, sr)\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # Try AMR-WB first, fall back to AAC if not available\n",
    "    encoders_to_try = [\n",
    "        ('libopencore-amrwb', 'amr'),  # AMR-WB decoder (read-only)\n",
    "        ('libvo_amrwbenc', 'amr'),      # AMR-WB encoder\n",
    "        ('aac', 'm4a'),                  # AAC fallback\n",
    "    ]\n",
    "    \n",
    "    for encoder, ext in encoders_to_try:\n",
    "        try:\n",
    "            # Adjust output path extension if needed\n",
    "            if ext != output_path.split('.')[-1]:\n",
    "                output_path_adj = output_path.rsplit('.', 1)[0] + '.' + ext\n",
    "            else:\n",
    "                output_path_adj = output_path\n",
    "            \n",
    "            # Compress\n",
    "            if encoder == 'aac':\n",
    "                # AAC doesn't need special sample rate\n",
    "                subprocess.run([\n",
    "                    'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',\n",
    "                    '-i', tmp_in_path,\n",
    "                    '-codec:a', encoder,\n",
    "                    '-b:a', f'{int(bitrate)}k',\n",
    "                    output_path_adj\n",
    "                ], check=True, capture_output=True, text=True)\n",
    "            else:\n",
    "                # AMR-WB requires 16kHz\n",
    "                bitrate_bps = int(bitrate * 1000)\n",
    "                subprocess.run([\n",
    "                    'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',\n",
    "                    '-i', tmp_in_path,\n",
    "                    '-ar', '16000',\n",
    "                    '-ac', '1',\n",
    "                    '-codec:a', encoder,\n",
    "                    '-b:a', str(bitrate_bps),\n",
    "                    output_path_adj\n",
    "                ], check=True, capture_output=True, text=True)\n",
    "            \n",
    "            # Decode back to WAV\n",
    "            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_out:\n",
    "                tmp_out_path = tmp_out.name\n",
    "                subprocess.run([\n",
    "                    'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',\n",
    "                    '-i', output_path_adj,\n",
    "                    tmp_out_path\n",
    "                ], check=True, capture_output=True, text=True)\n",
    "                \n",
    "                # Load decoded audio\n",
    "                decoded_audio, decoded_sr = librosa.load(tmp_out_path, sr=sr, mono=True)\n",
    "                \n",
    "                # Cleanup\n",
    "                if os.path.exists(tmp_out_path):\n",
    "                    os.remove(tmp_out_path)\n",
    "                if os.path.exists(tmp_in_path):\n",
    "                    os.remove(tmp_in_path)\n",
    "                \n",
    "                # Update output_path to actual path used\n",
    "                if output_path_adj != output_path:\n",
    "                    # Move file to expected location\n",
    "                    if os.path.exists(output_path_adj):\n",
    "                        import shutil\n",
    "                        shutil.move(output_path_adj, output_path)\n",
    "                \n",
    "                return decoded_audio\n",
    "                \n",
    "        except subprocess.CalledProcessError:\n",
    "            # Try next encoder\n",
    "            continue\n",
    "    \n",
    "    # All encoders failed\n",
    "    print(f\"AMR-WB compression failed: No suitable encoder found. Using original audio.\")\n",
    "    if os.path.exists(tmp_in_path):\n",
    "        os.remove(tmp_in_path)\n",
    "    return audio_array  # Return original on error\n",
    "\n",
    "print(\"Compression functions ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline function\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def torch_to_numpy(audio_tensor):\n",
    "    \"\"\"Convert torch tensor to numpy array.\"\"\"\n",
    "    if isinstance(audio_tensor, torch.Tensor):\n",
    "        audio_np = audio_tensor.cpu().numpy()\n",
    "        if audio_np.ndim > 1:\n",
    "            audio_np = audio_np.squeeze()\n",
    "        return audio_np\n",
    "    return audio_tensor\n",
    "\n",
    "def process_audio_file(audio_path: str, output_dir: str = \"/content/results\") -> Dict:\n",
    "    \"\"\"Process a single audio file through all pipelines.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {os.path.basename(audio_path)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    result = {\n",
    "        \"original_file\": os.path.basename(audio_path),\n",
    "        \"baseline_metrics\": {},\n",
    "        \"pgd_attack\": {\"opus\": {}, \"amr\": {}},\n",
    "        \"advreverb_attack\": {\"opus\": {}, \"amr\": {}}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load original audio\n",
    "        waveform = load_audio(audio_path)\n",
    "        original_np = torch_to_numpy(waveform)\n",
    "        \n",
    "        # Normalize for metrics computation\n",
    "        original_np = original_np / np.max(np.abs(original_np)) if np.max(np.abs(original_np)) > 0 else original_np\n",
    "        \n",
    "        # Compute baseline metrics (SNR is inf, PESQ/STOI are perfect)\n",
    "        print(\"\\n[1] Computing baseline metrics...\")\n",
    "        result[\"baseline_metrics\"] = {\n",
    "            \"snr\": float('inf'),\n",
    "            \"pesq\": 5.0,\n",
    "            \"stoi\": 1.0\n",
    "        }\n",
    "        print(f\"  Baseline: SNR=inf, PESQ=5.0, STOI=1.0\")\n",
    "        \n",
    "        # Get original transcript\n",
    "        original_transcript = transcribe_audio(original_np, TARGET_SR)\n",
    "        print(f\"  Original transcript: '{original_transcript}'\")\n",
    "        \n",
    "        # ========== PGD ATTACK PIPELINE ==========\n",
    "        print(\"\\n[2] PGD Attack Pipeline\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Apply PGD attack\n",
    "        print(\"  Applying PGD attack...\")\n",
    "        pgd_attacker = PGDAudioAttack(model, processor, sample_rate=TARGET_SR, device=device)\n",
    "        pgd_audio = pgd_attacker.forward(waveform)\n",
    "        pgd_np = torch_to_numpy(pgd_audio)\n",
    "        pgd_np = pgd_np / np.max(np.abs(pgd_np)) if np.max(np.abs(pgd_np)) > 0 else pgd_np\n",
    "        \n",
    "        # PGD → Opus\n",
    "        print(\"  Compressing to Opus...\")\n",
    "        opus_path = os.path.join(output_dir, f\"{Path(audio_path).stem}_pgd_opus.opus\")\n",
    "        pgd_opus_decoded = compress_opus(pgd_np, opus_path, TARGET_SR)\n",
    "        print(\"  Computing metrics on Opus-compressed audio...\")\n",
    "        pgd_opus_metrics = compute_all_metrics(original_np, pgd_opus_decoded, original_transcript, TARGET_SR)\n",
    "        result[\"pgd_attack\"][\"opus\"] = {\n",
    "            \"compressed_file\": os.path.basename(opus_path),\n",
    "            \"metrics\": pgd_opus_metrics\n",
    "        }\n",
    "        print(f\"    WER: {pgd_opus_metrics['wer']:.3f}, CER: {pgd_opus_metrics['cer']:.3f}\")\n",
    "        print(f\"    SNR: {pgd_opus_metrics['snr']:.2f} dB, PESQ: {pgd_opus_metrics['pesq']:.2f}, STOI: {pgd_opus_metrics['stoi']:.3f}\")\n",
    "        \n",
    "        # PGD → AMR-WB\n",
    "        print(\"  Compressing to AMR-WB...\")\n",
    "        amr_path = os.path.join(output_dir, f\"{Path(audio_path).stem}_pgd_amr.amr\")\n",
    "        pgd_amr_decoded = compress_amr(pgd_np, amr_path, TARGET_SR)\n",
    "        print(\"  Computing metrics on AMR-WB-compressed audio...\")\n",
    "        pgd_amr_metrics = compute_all_metrics(original_np, pgd_amr_decoded, original_transcript, TARGET_SR)\n",
    "        result[\"pgd_attack\"][\"amr\"] = {\n",
    "            \"compressed_file\": os.path.basename(amr_path),\n",
    "            \"metrics\": pgd_amr_metrics\n",
    "        }\n",
    "        print(f\"    WER: {pgd_amr_metrics['wer']:.3f}, CER: {pgd_amr_metrics['cer']:.3f}\")\n",
    "        print(f\"    SNR: {pgd_amr_metrics['snr']:.2f} dB, PESQ: {pgd_amr_metrics['pesq']:.2f}, STOI: {pgd_amr_metrics['stoi']:.3f}\")\n",
    "        \n",
    "        # ========== ADVREVERB ATTACK PIPELINE ==========\n",
    "        print(\"\\n[3] AdvReverb Attack Pipeline\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Apply AdvReverb attack\n",
    "        print(\"  Applying AdvReverb attack...\")\n",
    "        reverb_attacker = AdvReverbAttack(model, processor, sample_rate=TARGET_SR, device=device)\n",
    "        reverb_audio, _ = reverb_attacker.forward(waveform)\n",
    "        reverb_np = torch_to_numpy(reverb_audio)\n",
    "        reverb_np = reverb_np / np.max(np.abs(reverb_np)) if np.max(np.abs(reverb_np)) > 0 else reverb_np\n",
    "        \n",
    "        # AdvReverb → Opus\n",
    "        print(\"  Compressing to Opus...\")\n",
    "        opus_path = os.path.join(output_dir, f\"{Path(audio_path).stem}_advreverb_opus.opus\")\n",
    "        reverb_opus_decoded = compress_opus(reverb_np, opus_path, TARGET_SR)\n",
    "        print(\"  Computing metrics on Opus-compressed audio...\")\n",
    "        reverb_opus_metrics = compute_all_metrics(original_np, reverb_opus_decoded, original_transcript, TARGET_SR)\n",
    "        result[\"advreverb_attack\"][\"opus\"] = {\n",
    "            \"compressed_file\": os.path.basename(opus_path),\n",
    "            \"metrics\": reverb_opus_metrics\n",
    "        }\n",
    "        print(f\"    WER: {reverb_opus_metrics['wer']:.3f}, CER: {reverb_opus_metrics['cer']:.3f}\")\n",
    "        print(f\"    SNR: {reverb_opus_metrics['snr']:.2f} dB, PESQ: {reverb_opus_metrics['pesq']:.2f}, STOI: {reverb_opus_metrics['stoi']:.3f}\")\n",
    "        \n",
    "        # AdvReverb → AMR-WB\n",
    "        print(\"  Compressing to AMR-WB...\")\n",
    "        amr_path = os.path.join(output_dir, f\"{Path(audio_path).stem}_advreverb_amr.amr\")\n",
    "        reverb_amr_decoded = compress_amr(reverb_np, amr_path, TARGET_SR)\n",
    "        print(\"  Computing metrics on AMR-WB-compressed audio...\")\n",
    "        reverb_amr_metrics = compute_all_metrics(original_np, reverb_amr_decoded, original_transcript, TARGET_SR)\n",
    "        result[\"advreverb_attack\"][\"amr\"] = {\n",
    "            \"compressed_file\": os.path.basename(amr_path),\n",
    "            \"metrics\": reverb_amr_metrics\n",
    "        }\n",
    "        print(f\"    WER: {reverb_amr_metrics['wer']:.3f}, CER: {reverb_amr_metrics['cer']:.3f}\")\n",
    "        print(f\"    SNR: {reverb_amr_metrics['snr']:.2f} dB, PESQ: {reverb_amr_metrics['pesq']:.2f}, STOI: {reverb_amr_metrics['stoi']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n✓ Completed processing: {os.path.basename(audio_path)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error processing {audio_path}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        result[\"error\"] = str(e)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_all_files(audio_dir: str = AUDIO_DIR, output_json: str = \"/content/pipeline_results.json\"):\n",
    "    \"\"\"Process all audio files in directory.\"\"\"\n",
    "    wav_files = sorted(glob.glob(os.path.join(audio_dir, \"*.wav\")))\n",
    "    \n",
    "    if not wav_files:\n",
    "        print(f\"No .wav files found in {audio_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting batch processing of {len(wav_files)} files\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    all_results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, audio_path in enumerate(wav_files, 1):\n",
    "        print(f\"\\n\\n[{i}/{len(wav_files)}] Processing file...\")\n",
    "        result = process_audio_file(audio_path)\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save intermediate results periodically\n",
    "        if i % 10 == 0:\n",
    "            with open(output_json, 'w') as f:\n",
    "                json.dump(all_results, f, indent=2)\n",
    "            print(f\"\\n  Intermediate save: {i} files processed\")\n",
    "    \n",
    "    # Final save\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Batch processing complete!\")\n",
    "    print(f\"  Total files: {len(wav_files)}\")\n",
    "    print(f\"  Total time: {elapsed/60:.2f} minutes\")\n",
    "    print(f\"  Results saved to: {output_json}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "print(\"Pipeline functions ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline on all files\n",
    "process_all_files(AUDIO_DIR, \"/content/pipeline_results.json\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
